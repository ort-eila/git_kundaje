{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the files are on Anvil / Terra workspace \n",
    "# please run it from the workspace analysis to avoid bucket access permission issues\n",
    "# or use the gcloud login auth interface to authenticate yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "installations:\n",
    "\n",
    "1. pip install synapseclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "full_fragments_and_cell_type_labels = [(\"/Users/eilaarich-landkof-stanford/Documents/Code/git_kundaje_genomics/DNA/ATAC/single-cell/fragments-file-manipulation/synapse_in_out/fragment_files/syn52118181_syn52120036_ENCSR618WVK.atac.filter.fragments.hg38.tsv.gz/ENCSR618WVK.atac.filter.fragments.hg38.tsv.gz\",\n",
    "                                        \"/Users/eilaarich-landkof-stanford/Documents/Code/git_kundaje_genomics/DNA/ATAC/single-cell/fragments-file-manipulation/synapse_in_out/labels/syn34271785_ENCSR618WVK_syn34545411/cell_types.tsv.gz\")]\n",
    "# full_fragments_and_cell_type_labels = [(\"/Users/eilaarich-landkof-stanford/Downloads/k562/k562_err9847049_rna_atac_common_filtered_fragments.tsv.gz\",\n",
    "#                                         \"/Users/eilaarich-landkof-stanford/Downloads/k562/barcode_cell_types.tsv.gz\")]\n",
    "\n",
    "\n",
    "\n",
    "local_clusters_fld = os.path.join(os.getcwd(),\"clusters\")\n",
    "os.makedirs(local_clusters_fld, exist_ok=True)\n",
    "local_path_to_download = os.path.join(os.getcwd(),\"fragment_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_clusters_fld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path_to_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_file(filename):\n",
    "  print(\"remove_file method: {}\".format(filename))\n",
    "  if os.path.exists(filename):\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clusters_mapping(fld,atac_dataset_id,list_to_save):\n",
    " # Create a DataFrame from the tuple list\n",
    "    df = pd.DataFrame(list_to_save, columns=[\"ClusterId\", \"ManualAnnotationLabel\"])\n",
    "\n",
    "    # Specify the output TSV file path\n",
    "    output_tsv_file = os.path.join(fld,atac_dataset_id,\"{}_cluster_id_annotation_map.tsv\".format(atac_dataset_id))\n",
    "    print(\"sort_list_with_indices_and_save: output_tsv_file is {}\".format(output_tsv_file))\n",
    "    # Write the DataFrame to a TSV file\n",
    "    df.to_csv(output_tsv_file, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTNAT: we make sure to keep the same order if the cell_id and the cluster for the next execution\n",
    "def sort_list_with_indices(input_list):\n",
    "    sorted_list = sorted(input_list)\n",
    "    indexed_sorted_list = [(i, item) for i, item in enumerate(sorted_list)]\n",
    "\n",
    "    \n",
    "    return indexed_sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run tsv_files_utils_from_analysis.ipynb\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "for local_file_tuple in full_fragments_and_cell_type_labels:\n",
    "\n",
    "    # Dictionary to store the output file handles with names\n",
    "    output_handles = {}\n",
    "    local_fragment_file = local_file_tuple[0]\n",
    "    full_cell_types_annotation_file_path = local_file_tuple[1]\n",
    "    print(\"!!!!!local_fragment_file is {}\".format(local_fragment_file))\n",
    "    file_atac_dataset_id = local_fragment_file.split(\"_\")[-1].split(\".\")[0]\n",
    "    print(\"$$$$$$file_atac_dataset_id is {}\".format(file_atac_dataset_id))\n",
    " \n",
    "    # df_cell_types_for_atac_dataset = read_tsv_gz_to_dataframe_skipping_comments_and_empty_lines(full_cell_types_annotation_file_path)\n",
    "    # skip_rows = 1 - tanjin\n",
    "    skip_rows=1\n",
    "    df_cell_types_for_atac_dataset = read_tsv_gz_to_dataframe_skipping_empty_lines_and_rows(full_cell_types_annotation_file_path,skip_rows)\n",
    "    print(\"df_cell_types_for_atac_dataset.head(2) is {}\".format(df_cell_types_for_atac_dataset.head(2)))\n",
    "    cell_type_id_names_for_atac_dataset = list(set(df_cell_types_for_atac_dataset['cell_type_id'].to_list()))\n",
    "    sorted_idx_cell_type_id_names_for_atac_dataset = sort_list_with_indices(cell_type_id_names_for_atac_dataset)\n",
    "    print(\"%%%%%%sorted_idx_cell_type_id_names_for_atac_dataset is {}\".format(sorted_idx_cell_type_id_names_for_atac_dataset))\n",
    "    print(\"^^^^^^^number of cell_type_id_names_for_atac_dataset is {}\".format(len(sorted_idx_cell_type_id_names_for_atac_dataset)))\n",
    "\n",
    "    cell_id_to_type_dict = dict(zip(df_cell_types_for_atac_dataset['cell_id'], df_cell_types_for_atac_dataset['cell_type_id']))\n",
    "    print(\"\\n\".join([f\"Cell ID: {cell_id}, Cell Type ID: {cell_type_id}\" for cell_id, cell_type_id in islice(cell_id_to_type_dict.items(), 5)]))\n",
    "\n",
    "    # List of output text files with corresponding names\n",
    "    # [(\"output1.txt\", \"file_1\"), (\"output2.txt\", \"file_2\"), (\"output3.txt\", \"file_3\")]\n",
    "    output_tagAlign_files_with_names = [(os.path.join(local_clusters_fld,file_atac_dataset_id,\"{}_Cluster{}.tsv.tagAlign\".format(file_atac_dataset_id,item[0])),\n",
    "                                    item[1]) for item in sorted_idx_cell_type_id_names_for_atac_dataset]\n",
    "    print(\"@@@@output_tagAlign_files_with_names is {}\".format(output_tagAlign_files_with_names))\n",
    "\n",
    "    # this will make sure that we will not run the same tagAlign twice.\n",
    "    tagAlign_exists = [os.path.exists(output_tagAlign_file[0]) for output_tagAlign_file in output_tagAlign_files_with_names]\n",
    "    print(\"tagAlign_exists is {}\".format(tagAlign_exists))\n",
    "    if sum(tagAlign_exists) >0:\n",
    "        print(\"output_tagAlign_files_with_names {} is at work or was already downloaded. continue\".format(output_tagAlign_files_with_names))\n",
    "        continue # either started by annother process or already was processed\n",
    "    else:\n",
    "        print(\"!!!output_tagAlign_files_with_names {}. open files\".format(output_tagAlign_files_with_names))\n",
    "        for tag_file_path, tag_file_cell_type_name in output_tagAlign_files_with_names:\n",
    "            # print(\"tag_file_path is {}\".format(tag_file_path))\n",
    "            # print(\"os.path.dirname(tag_file_path) is {}\".format(os.path.dirname(tag_file_path)))\n",
    "            os.makedirs(os.path.dirname(tag_file_path), exist_ok=True)\n",
    "            output_handles[tag_file_cell_type_name] = open(tag_file_path, \"w\")\n",
    "\n",
    "    print(\"save the cluster to annotation mapping\")\n",
    "    save_clusters_mapping(local_clusters_fld,file_atac_dataset_id,sorted_idx_cell_type_id_names_for_atac_dataset)\n",
    "    \n",
    "    # print(\"!!!output_tagAlign_files_with_names {}\".format(output_tagAlign_files_with_names))\n",
    "    print(\"open local_fragment_file {}\".format(local_fragment_file))\n",
    "    with gzip.open(local_fragment_file, \"rt\") as infile:\n",
    "        missing_bc = 0\n",
    "        # Open the output files and store their handles in the list\n",
    "        num_of_lines_written=0\n",
    "        for line_number, line in enumerate(infile, start=1):\n",
    "            # # debug\n",
    "            # if line_number > 10:\n",
    "            #     continue \n",
    "\n",
    "            # here cases where bc_datasetId or datasetId_bc are being mixed between the fragments and the\n",
    "            # cell type are being address. you can select the righ out_list for your experiment\n",
    "            print(\"line is {}\".format(line))\n",
    "            out_list = split_fragment_line_string(line)\n",
    "            print(\"out_list is {}\".format(out_list))\n",
    "            bc = out_list[-1]\n",
    "            print(\"bc is {}\".format(bc))\n",
    "#             Austin output: chrom, start, end, bc, rem = line.rstrip('\\n').split('\\t', 5)\n",
    "            out_line_to_print = f\"{out_list[0]}\\t{out_list[1]}\\t{out_list[2]}\\t{bc}\\t{out_list[-2]}\\n\"\n",
    "#             chr1\t10007\t10175\tENCSR023FME#ENCSR023FME_GAAGGTTCAAAGTGTCAGTCAA\t1\n",
    "            num_of_lines_written +=1\n",
    "            returnTagAlign = convert_fragment_line_to_tagAlign(out_line_to_print)\n",
    "            \n",
    "            cell_id_to_type_dict[bc]\n",
    "            try:\n",
    "                tag_file_cell_type_id = cell_id_to_type_dict[bc] #df_cell_types_for_atac_dataset.loc[df_cell_types_for_atac_dataset['cell_id'] == bc, 'cell_type_id'].iloc[0]\n",
    "                # print(\"tag_file_cell_type_id is {}\".format(tag_file_cell_type_id))\n",
    "                output_handles[tag_file_cell_type_id].write(returnTagAlign)\n",
    "            except:\n",
    "                missing_bc+=1\n",
    "        \n",
    "        for tag_file_path, tag_file_cell_type_name in output_tagAlign_files_with_names:\n",
    "            print(\"tag_file_path is {}\".format(tag_file_path))\n",
    "            output_handles[tag_file_cell_type_name].close()    \n",
    "        print(\"finished clustering local_fragment_file {} by cell type. for types {}\".format(local_fragment_file, cell_type_id_names_for_atac_dataset))\n",
    "        print(\"total missing bc are {}\".format(missing_bc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_id_to_type_dict['ENCSR618WVK-1_AAAGGCTCACCAGGTT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cell_types_for_atac_dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_tagAlign_files_with_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_idx_cell_type_id_names_for_atac_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /Users/eilaarich-landkof-stanford/Documents/Code/git_kundaje_genomics/DNA/ATAC/single-cell/fragments-file-manipulation/synapse_in_out/clusters/\n",
    "\n",
    "\n",
    "# df_cell_types_for_atac_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_id_list = df_cell_types_for_atac_dataset['cell_id'].tolist()\n",
    "# cell_type_id_list = df_cell_types_for_atac_dataset['cell_type_id'].tolist()\n",
    "\n",
    "# print(\"Cell IDs List:\", cell_id_list[0:4])\n",
    "# print(\"Cell Type IDs List:\", cell_type_id_list[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_id_to_type_dict = dict(zip(df_cell_types_for_atac_dataset['cell_id'], df_cell_types_for_atac_dataset['cell_type_id']))\n",
    "# # cell_id_to_type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from itertools import islice\n",
    "\n",
    "# # Assuming your DataFrame is named df_cell_types_for_atac_dataset\n",
    "# # Replace df_cell_types_for_atac_dataset with the actual name of your DataFrame\n",
    "\n",
    "# cell_id_to_type_dict = dict(zip(df_cell_types_for_atac_dataset['cell_id'], df_cell_types_for_atac_dataset['cell_type_id']))\n",
    "\n",
    "# # Print the first 5 entries from the dictionary\n",
    "# num_entries_to_print = 5\n",
    "# print(\"\\n\".join([f\"Cell ID: {cell_id}, Cell Type ID: {cell_type_id}\" for cell_id, cell_type_id in islice(cell_id_to_type_dict.items(), num_entries_to_print)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_id_to_type_dict['GCCTGAdddddGGCTATGT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
